# transformers_demo
Built a  vanilla transformers model(Encoder-Decoder) structure for a translation task, to understand the core architecture concept and play around with changes 
will incorporate a custom learning rate decay in future
add a activation function in second layer of ffn and  benchmark the output
